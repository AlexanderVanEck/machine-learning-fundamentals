{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plot\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as stats_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('datasets/startups.csv')\n",
    "X = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, 4].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have categorical data, more than 1 possible value, so we need to\n",
    "# encode them to numerical data, and then one hot encode them to three\n",
    "# columns. Luckily we don't have to take care of the Dummy Variable Trap\n",
    "# here, since our LinearRegression library takes care of it.\n",
    "label_encoder = LabelEncoder()\n",
    "X[:, 3] = label_encoder.fit_transform(X[:, 3])\n",
    "one_hot_encoder = OneHotEncoder(categorical_features=[3])\n",
    "X = one_hot_encoder.fit_transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we split the data\n",
    "X_train, X_test,  y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have dominant dependenant variables, our categorical data is\n",
    "# 0...1 and our other data can become large. Thererfore we do feature\n",
    "# scaling, each feature will be in the same range. This step is optional\n",
    "# because our LinearRegression library takes care of it.\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we create a regressor and fit it to our training data\n",
    "regressor = LinearRegression()\n",
    "regressor = regressor.fit(X_train, y_train)\n",
    "\n",
    "# then we predict our dependant variable on the test set\n",
    "y_prediction = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The previous approach uses all features of the dataset, but what if not\n",
    "# all feature have a signifigant effect on the prediction. We can use\n",
    "# backwards elimination (among other) to find which features have a \n",
    "# significant effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to be able to use stats_model to find the signifigance value\n",
    "# of each feature (the P value) we need a feature (column) in our feature\n",
    "# set that represents are x0 variable, which is 1 for the bias we have in\n",
    "# our linear regression (y = b0*x0 + b1*x1 + ... bn*xn). This x0 variable\n",
    "# is called the intercept.\n",
    "# First we have to take care of the Dummy Variable Trap\n",
    "X = X[:, 1:]\n",
    "X_appended = np.append(arr=np.ones((50, 1)).astype(int), values=X, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with statistical signifigance: ['0', '3']\n",
      "Values with statistical signifigance: [[  1.00000000e+00   1.65349200e+05]\n",
      " [  1.00000000e+00   1.62597700e+05]\n",
      " [  1.00000000e+00   1.53441510e+05]\n",
      " [  1.00000000e+00   1.44372410e+05]\n",
      " [  1.00000000e+00   1.42107340e+05]\n",
      " [  1.00000000e+00   1.31876900e+05]\n",
      " [  1.00000000e+00   1.34615460e+05]\n",
      " [  1.00000000e+00   1.30298130e+05]\n",
      " [  1.00000000e+00   1.20542520e+05]\n",
      " [  1.00000000e+00   1.23334880e+05]\n",
      " [  1.00000000e+00   1.01913080e+05]\n",
      " [  1.00000000e+00   1.00671960e+05]\n",
      " [  1.00000000e+00   9.38637500e+04]\n",
      " [  1.00000000e+00   9.19923900e+04]\n",
      " [  1.00000000e+00   1.19943240e+05]\n",
      " [  1.00000000e+00   1.14523610e+05]\n",
      " [  1.00000000e+00   7.80131100e+04]\n",
      " [  1.00000000e+00   9.46571600e+04]\n",
      " [  1.00000000e+00   9.17491600e+04]\n",
      " [  1.00000000e+00   8.64197000e+04]\n",
      " [  1.00000000e+00   7.62538600e+04]\n",
      " [  1.00000000e+00   7.83894700e+04]\n",
      " [  1.00000000e+00   7.39945600e+04]\n",
      " [  1.00000000e+00   6.75325300e+04]\n",
      " [  1.00000000e+00   7.70440100e+04]\n",
      " [  1.00000000e+00   6.46647100e+04]\n",
      " [  1.00000000e+00   7.53288700e+04]\n",
      " [  1.00000000e+00   7.21076000e+04]\n",
      " [  1.00000000e+00   6.60515200e+04]\n",
      " [  1.00000000e+00   6.56054800e+04]\n",
      " [  1.00000000e+00   6.19944800e+04]\n",
      " [  1.00000000e+00   6.11363800e+04]\n",
      " [  1.00000000e+00   6.34088600e+04]\n",
      " [  1.00000000e+00   5.54939500e+04]\n",
      " [  1.00000000e+00   4.64260700e+04]\n",
      " [  1.00000000e+00   4.60140200e+04]\n",
      " [  1.00000000e+00   2.86637600e+04]\n",
      " [  1.00000000e+00   4.40699500e+04]\n",
      " [  1.00000000e+00   2.02295900e+04]\n",
      " [  1.00000000e+00   3.85585100e+04]\n",
      " [  1.00000000e+00   2.87543300e+04]\n",
      " [  1.00000000e+00   2.78929200e+04]\n",
      " [  1.00000000e+00   2.36409300e+04]\n",
      " [  1.00000000e+00   1.55057300e+04]\n",
      " [  1.00000000e+00   2.21777400e+04]\n",
      " [  1.00000000e+00   1.00023000e+03]\n",
      " [  1.00000000e+00   1.31546000e+03]\n",
      " [  1.00000000e+00   0.00000000e+00]\n",
      " [  1.00000000e+00   5.42050000e+02]\n",
      " [  1.00000000e+00   0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# X_optimal will contain only the variables that have a statistical effect\n",
    "# on the dependenant variable. We'll start with all of them and then work\n",
    "# backwards to find the significant ones.\n",
    "X_optimal = X_appended[:, [0, 1, 2, 3, 4, 5]]\n",
    "\n",
    "signifigance = 0.05\n",
    "highest_pvalue = 1\n",
    "columns = ['0', '1', '2', '3', '4', '5']\n",
    "while highest_pvalue > signifigance:\n",
    "    # we now initialise the Ordinary Least Squares class, which is the \n",
    "    # same formula as the LinearRegression class used above. And we fit\n",
    "    # it to the provided X_optimal (which contains all the feature)\n",
    "    # and the labels. We then loop over this to find the set of features\n",
    "    # that are statistically significant.\n",
    "    regressor = stats_model.OLS(endog=y, exog=X_optimal).fit()\n",
    "    highest_pvalue = regressor.pvalues.max()\n",
    "    highest_pvalue_index = np.where(regressor.pvalues==highest_pvalue)\n",
    "    if highest_pvalue > signifigance:\n",
    "        columns.pop(highest_pvalue_index[0][0])\n",
    "        X_optimal = np.delete(X_optimal, highest_pvalue_index, axis=1)\n",
    "        \n",
    "print('Columns with statistical signifigance: {}'.format(columns))\n",
    "print('Values with statistical signifigance: {}'.format(X_optimal))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
