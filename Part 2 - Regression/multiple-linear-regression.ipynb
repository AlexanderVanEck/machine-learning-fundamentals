{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plot\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as stats_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('datasets/startups.csv')\n",
    "X = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, 4].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.00000000e+00,   0.00000000e+00,   1.00000000e+00,\n",
       "          1.65349200e+05,   1.36897800e+05,   4.71784100e+05],\n",
       "       [  1.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          1.62597700e+05,   1.51377590e+05,   4.43898530e+05],\n",
       "       [  0.00000000e+00,   1.00000000e+00,   0.00000000e+00,\n",
       "          1.53441510e+05,   1.01145550e+05,   4.07934540e+05],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   1.00000000e+00,\n",
       "          1.44372410e+05,   1.18671850e+05,   3.83199620e+05],\n",
       "       [  0.00000000e+00,   1.00000000e+00,   0.00000000e+00,\n",
       "          1.42107340e+05,   9.13917700e+04,   3.66168420e+05],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   1.00000000e+00,\n",
       "          1.31876900e+05,   9.98147100e+04,   3.62861360e+05],\n",
       "       [  1.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          1.34615460e+05,   1.47198870e+05,   1.27716820e+05],\n",
       "       [  0.00000000e+00,   1.00000000e+00,   0.00000000e+00,\n",
       "          1.30298130e+05,   1.45530060e+05,   3.23876680e+05],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   1.00000000e+00,\n",
       "          1.20542520e+05,   1.48718950e+05,   3.11613290e+05],\n",
       "       [  1.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          1.23334880e+05,   1.08679170e+05,   3.04981620e+05],\n",
       "       [  0.00000000e+00,   1.00000000e+00,   0.00000000e+00,\n",
       "          1.01913080e+05,   1.10594110e+05,   2.29160950e+05],\n",
       "       [  1.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          1.00671960e+05,   9.17906100e+04,   2.49744550e+05],\n",
       "       [  0.00000000e+00,   1.00000000e+00,   0.00000000e+00,\n",
       "          9.38637500e+04,   1.27320380e+05,   2.49839440e+05],\n",
       "       [  1.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          9.19923900e+04,   1.35495070e+05,   2.52664930e+05],\n",
       "       [  0.00000000e+00,   1.00000000e+00,   0.00000000e+00,\n",
       "          1.19943240e+05,   1.56547420e+05,   2.56512920e+05],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   1.00000000e+00,\n",
       "          1.14523610e+05,   1.22616840e+05,   2.61776230e+05],\n",
       "       [  1.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          7.80131100e+04,   1.21597550e+05,   2.64346060e+05],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   1.00000000e+00,\n",
       "          9.46571600e+04,   1.45077580e+05,   2.82574310e+05],\n",
       "       [  0.00000000e+00,   1.00000000e+00,   0.00000000e+00,\n",
       "          9.17491600e+04,   1.14175790e+05,   2.94919570e+05],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   1.00000000e+00,\n",
       "          8.64197000e+04,   1.53514110e+05,   0.00000000e+00],\n",
       "       [  1.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          7.62538600e+04,   1.13867300e+05,   2.98664470e+05],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   1.00000000e+00,\n",
       "          7.83894700e+04,   1.53773430e+05,   2.99737290e+05],\n",
       "       [  0.00000000e+00,   1.00000000e+00,   0.00000000e+00,\n",
       "          7.39945600e+04,   1.22782750e+05,   3.03319260e+05],\n",
       "       [  0.00000000e+00,   1.00000000e+00,   0.00000000e+00,\n",
       "          6.75325300e+04,   1.05751030e+05,   3.04768730e+05],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   1.00000000e+00,\n",
       "          7.70440100e+04,   9.92813400e+04,   1.40574810e+05],\n",
       "       [  1.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          6.46647100e+04,   1.39553160e+05,   1.37962620e+05],\n",
       "       [  0.00000000e+00,   1.00000000e+00,   0.00000000e+00,\n",
       "          7.53288700e+04,   1.44135980e+05,   1.34050070e+05],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   1.00000000e+00,\n",
       "          7.21076000e+04,   1.27864550e+05,   3.53183810e+05],\n",
       "       [  0.00000000e+00,   1.00000000e+00,   0.00000000e+00,\n",
       "          6.60515200e+04,   1.82645560e+05,   1.18148200e+05],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   1.00000000e+00,\n",
       "          6.56054800e+04,   1.53032060e+05,   1.07138380e+05],\n",
       "       [  0.00000000e+00,   1.00000000e+00,   0.00000000e+00,\n",
       "          6.19944800e+04,   1.15641280e+05,   9.11312400e+04],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   1.00000000e+00,\n",
       "          6.11363800e+04,   1.52701920e+05,   8.82182300e+04],\n",
       "       [  1.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          6.34088600e+04,   1.29219610e+05,   4.60852500e+04],\n",
       "       [  0.00000000e+00,   1.00000000e+00,   0.00000000e+00,\n",
       "          5.54939500e+04,   1.03057490e+05,   2.14634810e+05],\n",
       "       [  1.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          4.64260700e+04,   1.57693920e+05,   2.10797670e+05],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   1.00000000e+00,\n",
       "          4.60140200e+04,   8.50474400e+04,   2.05517640e+05],\n",
       "       [  0.00000000e+00,   1.00000000e+00,   0.00000000e+00,\n",
       "          2.86637600e+04,   1.27056210e+05,   2.01126820e+05],\n",
       "       [  1.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          4.40699500e+04,   5.12831400e+04,   1.97029420e+05],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   1.00000000e+00,\n",
       "          2.02295900e+04,   6.59479300e+04,   1.85265100e+05],\n",
       "       [  1.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          3.85585100e+04,   8.29820900e+04,   1.74999300e+05],\n",
       "       [  1.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          2.87543300e+04,   1.18546050e+05,   1.72795670e+05],\n",
       "       [  0.00000000e+00,   1.00000000e+00,   0.00000000e+00,\n",
       "          2.78929200e+04,   8.47107700e+04,   1.64470710e+05],\n",
       "       [  1.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          2.36409300e+04,   9.61896300e+04,   1.48001110e+05],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   1.00000000e+00,\n",
       "          1.55057300e+04,   1.27382300e+05,   3.55341700e+04],\n",
       "       [  1.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          2.21777400e+04,   1.54806140e+05,   2.83347200e+04],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   1.00000000e+00,\n",
       "          1.00023000e+03,   1.24153040e+05,   1.90393000e+03],\n",
       "       [  0.00000000e+00,   1.00000000e+00,   0.00000000e+00,\n",
       "          1.31546000e+03,   1.15816210e+05,   2.97114460e+05],\n",
       "       [  1.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   1.35426920e+05,   0.00000000e+00],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   1.00000000e+00,\n",
       "          5.42050000e+02,   5.17431500e+04,   0.00000000e+00],\n",
       "       [  1.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   1.16983800e+05,   4.51730600e+04]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we have categorical data, more than 1 possible value, so we need to\n",
    "# encode them to numerical data, and then one hot encode them to three\n",
    "# columns. Luckily we don't have to take care of the Dummy Variable Trap\n",
    "# here, since our LinearRegression library takes care of it.\n",
    "label_encoder = LabelEncoder()\n",
    "X[:, 3] = label_encoder.fit_transform(X[:, 3])\n",
    "one_hot_encoder = OneHotEncoder(categorical_features=[3])\n",
    "X = one_hot_encoder.fit_transform(X).toarray()\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now we split the data\n",
    "X_train, X_test,  y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we have dominant dependenant variables, our categorical data is\n",
    "# 0...1 and our other data can become large. Thererfore we do feature\n",
    "# scaling, each feature will be in the same range. This step is optional\n",
    "# because our LinearRegression library takes care of it.\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Alexander/Virtualenvs/ml-data-processing/lib/python3.6/site-packages/scipy/linalg/basic.py:1018: RuntimeWarning: internal gelsd driver lwork query error, required iwork dimension not returned. This is likely the result of LAPACK bug 0038, fixed in LAPACK 3.2.2 (released July 21, 2010). Falling back to 'gelss' driver.\n",
      "  warnings.warn(mesg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "# now we create a regressor and fit it to our training data\n",
    "regressor = LinearRegression()\n",
    "regressor = regressor.fit(X_train, y_train)\n",
    "\n",
    "# then we predict our dependant variable on the test set\n",
    "y_prediction = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The previous approach uses all features of the dataset, but what if not\n",
    "# all feature have a signifigant effect on the prediction. We can use\n",
    "# backwards elimination (among other) to find which features have a \n",
    "# significant effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In order to be able to use stats_model to find the signifigance value\n",
    "# of each feature (the P value) we need a feature (column) in our feature\n",
    "# set that represents are x0 variable, which is 1 for the bias we have in\n",
    "# our linear regression (y = b0*x0 + b1*x1 + ... bn*xn). This x0 variable\n",
    "# is called the intercept.\n",
    "# First we have to take care of the Dummy Variable Trap\n",
    "X = X[:, 1:]\n",
    "X_appended = np.append(arr=np.ones((50, 1)).astype(int), values=X, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with statistical signifigance: ['0', '3']\n",
      "Values with statistical signifigance: [[  1.00000000e+00   1.65349200e+05]\n",
      " [  1.00000000e+00   1.62597700e+05]\n",
      " [  1.00000000e+00   1.53441510e+05]\n",
      " [  1.00000000e+00   1.44372410e+05]\n",
      " [  1.00000000e+00   1.42107340e+05]\n",
      " [  1.00000000e+00   1.31876900e+05]\n",
      " [  1.00000000e+00   1.34615460e+05]\n",
      " [  1.00000000e+00   1.30298130e+05]\n",
      " [  1.00000000e+00   1.20542520e+05]\n",
      " [  1.00000000e+00   1.23334880e+05]\n",
      " [  1.00000000e+00   1.01913080e+05]\n",
      " [  1.00000000e+00   1.00671960e+05]\n",
      " [  1.00000000e+00   9.38637500e+04]\n",
      " [  1.00000000e+00   9.19923900e+04]\n",
      " [  1.00000000e+00   1.19943240e+05]\n",
      " [  1.00000000e+00   1.14523610e+05]\n",
      " [  1.00000000e+00   7.80131100e+04]\n",
      " [  1.00000000e+00   9.46571600e+04]\n",
      " [  1.00000000e+00   9.17491600e+04]\n",
      " [  1.00000000e+00   8.64197000e+04]\n",
      " [  1.00000000e+00   7.62538600e+04]\n",
      " [  1.00000000e+00   7.83894700e+04]\n",
      " [  1.00000000e+00   7.39945600e+04]\n",
      " [  1.00000000e+00   6.75325300e+04]\n",
      " [  1.00000000e+00   7.70440100e+04]\n",
      " [  1.00000000e+00   6.46647100e+04]\n",
      " [  1.00000000e+00   7.53288700e+04]\n",
      " [  1.00000000e+00   7.21076000e+04]\n",
      " [  1.00000000e+00   6.60515200e+04]\n",
      " [  1.00000000e+00   6.56054800e+04]\n",
      " [  1.00000000e+00   6.19944800e+04]\n",
      " [  1.00000000e+00   6.11363800e+04]\n",
      " [  1.00000000e+00   6.34088600e+04]\n",
      " [  1.00000000e+00   5.54939500e+04]\n",
      " [  1.00000000e+00   4.64260700e+04]\n",
      " [  1.00000000e+00   4.60140200e+04]\n",
      " [  1.00000000e+00   2.86637600e+04]\n",
      " [  1.00000000e+00   4.40699500e+04]\n",
      " [  1.00000000e+00   2.02295900e+04]\n",
      " [  1.00000000e+00   3.85585100e+04]\n",
      " [  1.00000000e+00   2.87543300e+04]\n",
      " [  1.00000000e+00   2.78929200e+04]\n",
      " [  1.00000000e+00   2.36409300e+04]\n",
      " [  1.00000000e+00   1.55057300e+04]\n",
      " [  1.00000000e+00   2.21777400e+04]\n",
      " [  1.00000000e+00   1.00023000e+03]\n",
      " [  1.00000000e+00   1.31546000e+03]\n",
      " [  1.00000000e+00   0.00000000e+00]\n",
      " [  1.00000000e+00   5.42050000e+02]\n",
      " [  1.00000000e+00   0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# X_optimal will contain only the variables that have a statistical effect\n",
    "# on the dependenant variable. We'll start with all of them and then work\n",
    "# backwards to find the significant ones.\n",
    "X_optimal = X_appended[:, [0, 1, 2, 3, 4, 5]]\n",
    "\n",
    "signifigance = 0.05\n",
    "highest_pvalue = 1\n",
    "columns = ['0', '1', '2', '3', '4', '5']\n",
    "while highest_pvalue > signifigance:\n",
    "    # we now initialise the Ordinary Least Squares class, which is the \n",
    "    # same formula as the LinearRegression class used above. And we fit\n",
    "    # it to the provided X_optimal (which contains all the feature)\n",
    "    # and the labels. We then loop over this to find the set of features\n",
    "    # that are statistically significant.\n",
    "    regressor = stats_model.OLS(endog=y, exog=X_optimal).fit()\n",
    "    highest_pvalue = regressor.pvalues.max()\n",
    "    highest_pvalue_index = np.where(regressor.pvalues==highest_pvalue)\n",
    "    if highest_pvalue > signifigance:\n",
    "        columns.pop(highest_pvalue_index[0][0])\n",
    "        X_optimal = np.delete(X_optimal, highest_pvalue_index, axis=1)\n",
    "        \n",
    "print('Columns with statistical signifigance: {}'.format(columns))\n",
    "print('Values with statistical signifigance: {}'.format(X_optimal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
